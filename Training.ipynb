{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Training.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOdCIK61Im1yEo2IF2kclR2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LSyhSk9y5p8Y","executionInfo":{"status":"ok","timestamp":1655909606457,"user_tz":-420,"elapsed":16328,"user":{"displayName":"Hữu Phương Nguyễn","userId":"15386454328741485337"}},"outputId":"445bf727-b148-4cf6-b8c7-8160a6ec744e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/DO_AN/Dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5vZHcuQa5yHA","executionInfo":{"status":"ok","timestamp":1655909606457,"user_tz":-420,"elapsed":14,"user":{"displayName":"Hữu Phương Nguyễn","userId":"15386454328741485337"}},"outputId":"6fb5451d-7a04-4353-9930-830dbb462734"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/DO_AN/Dataset\n"]}]},{"cell_type":"code","source":["!nvidia-smi\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HswgJHj1v7QG","executionInfo":{"status":"ok","timestamp":1655909606458,"user_tz":-420,"elapsed":11,"user":{"displayName":"Hữu Phương Nguyễn","userId":"15386454328741485337"}},"outputId":"04cca186-aab9-4a9e-d97c-5531b67870d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Jun 22 14:53:26 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   48C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["import torch\n","torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cf1PdJ1NwElx","executionInfo":{"status":"ok","timestamp":1655909609626,"user_tz":-420,"elapsed":3174,"user":{"displayName":"Hữu Phương Nguyễn","userId":"15386454328741485337"}},"outputId":"375cb812-55bf-4e50-cd0a-f0481c771f24"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import math\n","import os\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","from functools import reduce\n","\n","import cv2\n","import numpy as np\n","import scipy\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from scipy.ndimage import grey_dilation, grey_erosion\n","from scipy.ndimage import morphology\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import copy\n","import logging\n","import shutil"],"metadata":{"id":"SrxT4S1j61up"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logging.basicConfig(filename='Default.log', level=logging.INFO, datefmt='%Y-%m-%d %H:%M:%S')\n","logging.info('--------------------------------')\n","# ----------------------------------------------------------------------------------\n","# Tool Classes/Functions\n","# ----------------------------------------------------------------------------------\n","torch_transforms = transforms.Compose(\n","    [\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ]\n",")\n"],"metadata":{"id":"qfOKRhFec_YG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _make_divisible(v, divisor, min_value=None):\n","    if min_value is None:\n","        min_value = divisor\n","    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n","    # Make sure that round down does not go down by more than 10%.\n","    if new_v < 0.9 * v:\n","        new_v += divisor\n","    return new_v\n","    "],"metadata":{"id":"4GilfaTrdHZU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def conv_bn(inp, oup, stride):\n","    return nn.Sequential(\n","        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n","        nn.BatchNorm2d(oup),\n","        nn.ReLU6(inplace=True)\n","    )"],"metadata":{"id":"9KElold_dMtk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def conv_1x1_bn(inp, oup):\n","    return nn.Sequential(\n","        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n","        nn.BatchNorm2d(oup),\n","        nn.ReLU6(inplace=True)\n","    )"],"metadata":{"id":"ei51ma47dOpU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class InvertedResidual(nn.Module):\n","    def __init__(self, inp, oup, stride, expansion, dilation=1):\n","        super(InvertedResidual, self).__init__()\n","        self.stride = stride\n","        assert stride in [1, 2]\n","\n","        hidden_dim = round(inp * expansion)\n","        self.use_res_connect = self.stride == 1 and inp == oup\n","\n","        if expansion == 1:\n","            self.conv = nn.Sequential(\n","                # dw\n","                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, dilation=dilation, bias=False),\n","                nn.BatchNorm2d(hidden_dim),\n","                nn.ReLU6(inplace=True),\n","                # pw-linear\n","                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n","                nn.BatchNorm2d(oup),\n","            )\n","        else:\n","            self.conv = nn.Sequential(\n","                # pw\n","                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n","                nn.BatchNorm2d(hidden_dim),\n","                nn.ReLU6(inplace=True),\n","                # dw\n","                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, dilation=dilation, bias=False),\n","                nn.BatchNorm2d(hidden_dim),\n","                nn.ReLU6(inplace=True),\n","                # pw-linear\n","                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n","                nn.BatchNorm2d(oup),\n","            )\n","\n","    def forward(self, x):\n","        if self.use_res_connect:\n","            return x + self.conv(x)\n","        else:\n","            return self.conv(x)"],"metadata":{"id":"4inxCY5AdQOK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MobileNetV2(nn.Module):\n","    def __init__(self, in_channels, alpha=1.0, expansion=6, num_classes=1000):\n","        super(MobileNetV2, self).__init__()\n","        self.in_channels = in_channels\n","        self.num_classes = num_classes\n","        input_channel = 32\n","        last_channel = 1280\n","        interverted_residual_setting = [\n","            # t, c, n, s\n","            [1, 16, 1, 1],\n","            [expansion, 24, 2, 2],\n","            [expansion, 32, 3, 2],\n","            [expansion, 64, 4, 2],\n","            [expansion, 96, 3, 1],\n","            [expansion, 160, 3, 2],\n","            [expansion, 320, 1, 1],\n","        ]\n","\n","        # building first layer\n","        input_channel = _make_divisible(input_channel * alpha, 8)\n","        self.last_channel = _make_divisible(last_channel * alpha, 8) if alpha > 1.0 else last_channel\n","        self.features = [conv_bn(self.in_channels, input_channel, 2)]\n","\n","        # building inverted residual blocks\n","        for t, c, n, s in interverted_residual_setting:\n","            output_channel = _make_divisible(int(c * alpha), 8)\n","            for i in range(n):\n","                if i == 0:\n","                    self.features.append(InvertedResidual(input_channel, output_channel, s, expansion=t))\n","                else:\n","                    self.features.append(InvertedResidual(input_channel, output_channel, 1, expansion=t))\n","                input_channel = output_channel\n","\n","        # building last several layers\n","        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n","\n","        # make it nn.Sequential\n","        self.features = nn.Sequential(*self.features)\n","\n","        # building classifier\n","        if self.num_classes is not None:\n","            self.classifier = nn.Sequential(\n","                nn.Dropout(0.2),\n","                nn.Linear(self.last_channel, num_classes),\n","            )\n","\n","        # Initialize weights\n","        self._init_weights()\n","\n","    def forward(self, x, feature_names=None):\n","        # Stage1\n","        x = reduce(lambda x, n: self.features[n](x), list(range(0, 2)), x)\n","        # Stage2\n","        x = reduce(lambda x, n: self.features[n](x), list(range(2, 4)), x)\n","        # Stage3\n","        x = reduce(lambda x, n: self.features[n](x), list(range(4, 7)), x)\n","        # Stage4\n","        x = reduce(lambda x, n: self.features[n](x), list(range(7, 14)), x)\n","        # Stage5\n","        x = reduce(lambda x, n: self.features[n](x), list(range(14, 19)), x)\n","\n","        # Classification\n","        if self.num_classes is not None:\n","            x = x.mean(dim=(2, 3))\n","            x = self.classifier(x)\n","\n","        # Output\n","        return x\n","\n","    def _load_pretrained_model(self, pretrained_file):\n","        pretrain_dict = torch.load(pretrained_file, map_location='cpu')\n","        model_dict = {}\n","        state_dict = self.state_dict()\n","        print(\"[MobileNetV2] Loading pretrained model...\")\n","        for k, v in pretrain_dict.items():\n","            if k in state_dict:\n","                model_dict[k] = v\n","            else:\n","                print(k, \"is ignored\")\n","        state_dict.update(model_dict)\n","        self.load_state_dict(state_dict)\n","\n","    def _init_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","            elif isinstance(m, nn.Linear):\n","                n = m.weight.size(1)\n","                m.weight.data.normal_(0, 0.01)\n","                m.bias.data.zero_()\n"],"metadata":{"id":"HSnLhp7BdSmo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class IBNorm(nn.Module):\n","    \"\"\" Combine Instance Norm and Batch Norm into One Layer\n","    \"\"\"\n","\n","    def __init__(self, in_channels):\n","        super(IBNorm, self).__init__()\n","        in_channels = in_channels\n","        self.bnorm_channels = int(in_channels / 2)\n","        self.inorm_channels = in_channels - self.bnorm_channels\n","\n","        self.bnorm = nn.BatchNorm2d(self.bnorm_channels, affine=True)\n","        self.inorm = nn.InstanceNorm2d(self.inorm_channels, affine=False)\n","\n","    def forward(self, x):\n","        bn_x = self.bnorm(x[:, :self.bnorm_channels, ...].contiguous())\n","        in_x = self.inorm(x[:, self.bnorm_channels:, ...].contiguous())\n","\n","        return torch.cat((bn_x, in_x), 1)"],"metadata":{"id":"QfyrmPgsdWW-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Conv2dIBNormRelu(nn.Module):\n","    \"\"\" Convolution + IBNorm + ReLu\n","    \"\"\"\n","\n","    def __init__(self, in_channels, out_channels, kernel_size,\n","                 stride=1, padding=0, dilation=1, groups=1, bias=True,\n","                 with_ibn=True, with_relu=True):\n","        super(Conv2dIBNormRelu, self).__init__()\n","\n","        layers = [\n","            nn.Conv2d(in_channels, out_channels, kernel_size,\n","                      stride=stride, padding=padding, dilation=dilation,\n","                      groups=groups, bias=bias)\n","        ]\n","\n","        if with_ibn:\n","            layers.append(IBNorm(out_channels))\n","        if with_relu:\n","            layers.append(nn.ReLU(inplace=True))\n","\n","        self.layers = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.layers(x)\n"],"metadata":{"id":"yQS6wH1YdcJy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BaseBackbone(nn.Module):\n","    \"\"\" Superclass of Replaceable Backbone Model for Semantic Estimation\n","    \"\"\"\n","\n","    def __init__(self, in_channels):\n","        super(BaseBackbone, self).__init__()\n","        self.in_channels = in_channels\n","\n","        self.model = None\n","        self.enc_channels = []\n","\n","    def forward(self, x):\n","        raise NotImplementedError\n","\n","    def load_pretrained_ckpt(self):\n","        raise NotImplementedError\n"],"metadata":{"id":"tYOKb42GdeDQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MobileNetV2Backbone(BaseBackbone):\n","    \"\"\" MobileNetV2 Backbone\n","    \"\"\"\n","\n","    def __init__(self, in_channels):\n","        super(MobileNetV2Backbone, self).__init__(in_channels)\n","        self.model = MobileNetV2(self.in_channels, alpha=1.0, expansion=6, num_classes=None)\n","        self.enc_channels = [16, 24, 32, 96, 1280]\n","\n","    def forward(self, x):\n","        x = reduce(lambda x, n: self.model.features[n](x), list(range(0, 2)), x)\n","        enc2x = x\n","        x = reduce(lambda x, n: self.model.features[n](x), list(range(2, 4)), x)\n","        enc4x = x\n","        x = reduce(lambda x, n: self.model.features[n](x), list(range(4, 7)), x)\n","        enc8x = x\n","        x = reduce(lambda x, n: self.model.features[n](x), list(range(7, 14)), x)\n","        enc16x = x\n","        x = reduce(lambda x, n: self.model.features[n](x), list(range(14, 19)), x)\n","        enc32x = x\n","        return [enc2x, enc4x, enc8x, enc16x, enc32x]\n","\n","    def load_pretrained_ckpt(self):\n","        ckpt_path = './mobilenetv2_human_seg.ckpt'\n","        if not os.path.exists(ckpt_path):\n","            print('cannot find the pretrained mobilenetv2 backbone')\n","            exit()\n","\n","        ckpt = torch.load(ckpt_path)\n","        self.model.load_state_dict(ckpt)"],"metadata":{"id":"5ouXd5g4dfk6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SEBlock(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels, reduction=1):\n","        super(SEBlock, self).__init__()\n","        self.pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc = nn.Sequential(\n","            nn.Linear(in_channels, int(in_channels // reduction), bias=False),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(int(in_channels // reduction), out_channels, bias=False),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        b, c, _, _ = x.size()\n","        w = self.pool(x).view(b, c)\n","        w = self.fc(w).view(b, c, 1, 1)\n","\n","        return x * w.expand_as(x)"],"metadata":{"id":"GMQzglG5dh9B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LRBranch(nn.Module):\n","    \"\"\" Low Resolution Branch of MODNet\n","    \"\"\"\n","\n","    def __init__(self, backbone):\n","        super(LRBranch, self).__init__()\n","\n","        enc_channels = backbone.enc_channels\n","        self.backbone = backbone\n","        self.se_block = SEBlock(enc_channels[4], enc_channels[4], reduction=4)\n","        self.conv_lr16x = Conv2dIBNormRelu(enc_channels[4], enc_channels[3], 5, stride=1, padding=2)\n","        self.conv_lr8x = Conv2dIBNormRelu(enc_channels[3], enc_channels[2], 5, stride=1, padding=2)\n","        self.conv_lr = Conv2dIBNormRelu(enc_channels[2], 1, kernel_size=3, stride=2, padding=1, with_ibn=False,\n","                                        with_relu=False)\n","\n","    def forward(self, img, inference):\n","        enc_features = self.backbone.forward(img)\n","        enc2x, enc4x, enc32x = enc_features[0], enc_features[1], enc_features[4]\n","\n","        enc32x = self.se_block(enc32x)\n","        lr16x = F.interpolate(enc32x, scale_factor=2, mode='bilinear', align_corners=False)\n","        lr16x = self.conv_lr16x(lr16x)\n","\n","        lr8x = F.interpolate(lr16x, scale_factor=2, mode='bilinear', align_corners=False)\n","        lr8x = self.conv_lr8x(lr8x)\n","        pred_semantic = None\n","        if not inference:\n","            lr = self.conv_lr(lr8x)\n","            pred_semantic = torch.sigmoid(lr)\n","        return pred_semantic, lr8x, [enc2x, enc4x]"],"metadata":{"id":"W_XRoEy6dj88"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class HRBranch(nn.Module):\n","    \"\"\" High Resolution Branch of MODNet\n","    \"\"\"\n","\n","    def __init__(self, hr_channels, enc_channels):\n","        super(HRBranch, self).__init__()\n","\n","        self.tohr_enc2x = Conv2dIBNormRelu(enc_channels[0], hr_channels, 1, stride=1, padding=0)\n","        self.conv_enc2x = Conv2dIBNormRelu(hr_channels + 3, hr_channels, 3, stride=2, padding=1)\n","\n","        self.tohr_enc4x = Conv2dIBNormRelu(enc_channels[1], hr_channels, 1, stride=1, padding=0)\n","        self.conv_enc4x = Conv2dIBNormRelu(2 * hr_channels, 2 * hr_channels, 3, stride=1, padding=1)\n","\n","        self.conv_hr4x = nn.Sequential(\n","            Conv2dIBNormRelu(3 * hr_channels + 3, 2 * hr_channels, 3, stride=1, padding=1),\n","            Conv2dIBNormRelu(2 * hr_channels, 2 * hr_channels, 3, stride=1, padding=1),\n","            Conv2dIBNormRelu(2 * hr_channels, hr_channels, 3, stride=1, padding=1),\n","        )\n","\n","        self.conv_hr2x = nn.Sequential(\n","            Conv2dIBNormRelu(2 * hr_channels, 2 * hr_channels, 3, stride=1, padding=1),\n","            Conv2dIBNormRelu(2 * hr_channels, hr_channels, 3, stride=1, padding=1),\n","            Conv2dIBNormRelu(hr_channels, hr_channels, 3, stride=1, padding=1),\n","            Conv2dIBNormRelu(hr_channels, hr_channels, 3, stride=1, padding=1),\n","        )\n","\n","        self.conv_hr = nn.Sequential(\n","            Conv2dIBNormRelu(hr_channels + 3, hr_channels, 3, stride=1, padding=1),\n","            Conv2dIBNormRelu(hr_channels, 1, kernel_size=1, stride=1, padding=0, with_ibn=False, with_relu=False),\n","        )\n","\n","    def forward(self, img, enc2x, enc4x, lr8x, inference):\n","        img2x = F.interpolate(img, scale_factor=1 / 2, mode='bilinear', align_corners=False)\n","        img4x = F.interpolate(img, scale_factor=1 / 4, mode='bilinear', align_corners=False)\n","\n","        enc2x = self.tohr_enc2x(enc2x)\n","        hr4x = self.conv_enc2x(torch.cat((img2x, enc2x), dim=1))\n","\n","        enc4x = self.tohr_enc4x(enc4x)\n","        hr4x = self.conv_enc4x(torch.cat((hr4x, enc4x), dim=1))\n","        lr4x = F.interpolate(lr8x, scale_factor=2, mode='bilinear', align_corners=False)\n","        hr4x = self.conv_hr4x(torch.cat((hr4x, lr4x, img4x), dim=1))\n","\n","        hr2x = F.interpolate(hr4x, scale_factor=2, mode='bilinear', align_corners=False)\n","        hr2x = self.conv_hr2x(torch.cat((hr2x, enc2x), dim=1))\n","\n","        pred_detail = None\n","        if not inference:\n","            hr = F.interpolate(hr2x, scale_factor=2, mode='bilinear', align_corners=False)\n","            hr = self.conv_hr(torch.cat((hr, img), dim=1))\n","            pred_detail = torch.sigmoid(hr)\n","\n","        return pred_detail, hr2x"],"metadata":{"id":"lmOLIH-odlkn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FusionBranch(nn.Module):\n","    \"\"\" Fusion Branch of MODNet\n","    \"\"\"\n","\n","    def __init__(self, hr_channels, enc_channels):\n","        super(FusionBranch, self).__init__()\n","        self.conv_lr4x = Conv2dIBNormRelu(enc_channels[2], hr_channels, 5, stride=1, padding=2)\n","\n","        self.conv_f2x = Conv2dIBNormRelu(2 * hr_channels, hr_channels, 3, stride=1, padding=1)\n","        self.conv_f = nn.Sequential(\n","            Conv2dIBNormRelu(hr_channels + 3, int(hr_channels / 2), 3, stride=1, padding=1),\n","            Conv2dIBNormRelu(int(hr_channels / 2), 1, 1, stride=1, padding=0, with_ibn=False, with_relu=False),\n","        )\n","\n","    def forward(self, img, lr8x, hr2x):\n","        lr4x = F.interpolate(lr8x, scale_factor=2, mode='bilinear', align_corners=False)\n","        lr4x = self.conv_lr4x(lr4x)\n","        lr2x = F.interpolate(lr4x, scale_factor=2, mode='bilinear', align_corners=False)\n","\n","        f2x = self.conv_f2x(torch.cat((lr2x, hr2x), dim=1))\n","        f = F.interpolate(f2x, scale_factor=2, mode='bilinear', align_corners=False)\n","        f = self.conv_f(torch.cat((f, img), dim=1))\n","        pred_matte = torch.sigmoid(f)\n","\n","        return pred_matte"],"metadata":{"id":"IctarbDNdo6Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MODNet(nn.Module):\n","    \"\"\" Architecture of MODNet\n","    \"\"\"\n","\n","    def __init__(self, in_channels=3, hr_channels=32, backbone_arch='mobilenetv2', backbone_pretrained=True):\n","        super(MODNet, self).__init__()\n","\n","        self.in_channels = in_channels\n","        self.hr_channels = hr_channels\n","        self.backbone_arch = backbone_arch\n","        self.backbone_pretrained = backbone_pretrained\n","\n","        self.backbone = MobileNetV2Backbone(self.in_channels)\n","\n","        self.lr_branch = LRBranch(self.backbone)\n","        self.hr_branch = HRBranch(self.hr_channels, self.backbone.enc_channels)\n","        self.f_branch = FusionBranch(self.hr_channels, self.backbone.enc_channels)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                self._init_conv(m)\n","            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.InstanceNorm2d):\n","                self._init_norm(m)\n","\n","        if self.backbone_pretrained:\n","            self.backbone.load_pretrained_ckpt()\n","\n","    def forward(self, img, inference):\n","        pred_semantic, lr8x, [enc2x, enc4x] = self.lr_branch(img, inference)\n","\n","        pred_detail, hr2x = self.hr_branch(img, enc2x, enc4x, lr8x, inference)\n","        pred_matte = self.f_branch(img, lr8x, hr2x)\n","\n","        return pred_semantic, pred_detail, pred_matte\n","\n","    def freeze_norm(self):\n","        norm_types = [nn.BatchNorm2d, nn.InstanceNorm2d]\n","        for m in self.modules():\n","            for n in norm_types:\n","                if isinstance(m, n):\n","                    m.eval()\n","                    continue\n","\n","    def _init_conv(self, conv):\n","        nn.init.kaiming_uniform_(\n","            conv.weight, a=0, mode='fan_in', nonlinearity='relu')\n","        if conv.bias is not None:\n","            nn.init.constant_(conv.bias, 0)\n","\n","    def _init_norm(self, norm):\n","        if norm.weight is not None:\n","            nn.init.constant_(norm.weight, 1)\n","            nn.init.constant_(norm.bias, 0)"],"metadata":{"id":"F8xm8OlCdqt2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GaussianBlurLayer(nn.Module):\n","    \"\"\" Add Gaussian Blur to a 4D tensors\n","    This layer takes a 4D tensor of {N, C, H, W} as input.\n","    The Gaussian blur will be performed in given channel number (C) splitly.\n","    \"\"\"\n","\n","    def __init__(self, channels, kernel_size):\n","        \"\"\"\n","        Arguments:\n","            channels (int): Channel for input tensor\n","            kernel_size (int): Size of the kernel used in blurring\n","        \"\"\"\n","\n","        super(GaussianBlurLayer, self).__init__()\n","        self.channels = channels\n","        self.kernel_size = kernel_size\n","        assert self.kernel_size % 2 != 0\n","\n","        self.op = nn.Sequential(\n","            nn.ReflectionPad2d(math.floor(self.kernel_size / 2)),\n","            nn.Conv2d(channels, channels, self.kernel_size,\n","                      stride=1, padding=0, bias=None, groups=channels)\n","        )\n","\n","        self._init_kernel()\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Arguments:\n","            x (torch.Tensor): input 4D tensor\n","        Returns:\n","            torch.Tensor: Blurred version of the input\n","        \"\"\"\n","        if not len(list(x.shape)) == 4:\n","            print('\\'GaussianBlurLayer\\' requires a 4D tensor as input\\n')\n","            exit()\n","        elif not x.shape[1] == self.channels:\n","            print('In \\'GaussianBlurLayer\\', the required channel ({0}) is'\n","                  'not the same as input ({1})\\n'.format(self.channels, x.shape[1]))\n","            exit()\n","\n","        return self.op(x)\n","\n","    def _init_kernel(self):\n","        sigma = 0.3 * ((self.kernel_size - 1) * 0.5 - 1) + 0.8\n","\n","        n = np.zeros((self.kernel_size, self.kernel_size))\n","        i = math.floor(self.kernel_size / 2)\n","        n[i, i] = 1\n","        kernel = scipy.ndimage.gaussian_filter(n, sigma)\n","\n","        for name, param in self.named_parameters():\n","            param.data.copy_(torch.from_numpy(kernel))"],"metadata":{"id":"fF7V0ZXRdsuL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ImagesDataset(Dataset):\n","    \"\"\"because of i want to use the modle author provided, i use the same size as the val in demo,\n","    if you want to change the size, change the size in code func: __getitem__, attention the shape\"\"\"\n","    def __init__(self, root, transform=None, w=1024, h=576):\n","        self.root = root\n","        self.transform = transform\n","        self.tensor = transforms.Compose([transforms.ToTensor()])\n","        self.w = w\n","        self.h = h\n","        self.imgs = sorted(os.listdir(os.path.join(self.root, './train/blurred_image')))\n","        self.alphas = sorted(os.listdir(os.path.join(self.root, './train/mask')))\n","        assert len(self.imgs) == len(self.alphas), 'the number of dataset is different, please check it.'\n","\n","    def getTrimap(self, alpha):\n","        fg = np.array(np.equal(alpha, 255).astype(np.float32))\n","        unknown = np.array(np.not_equal(alpha, 0).astype(np.float32))  # unknown = alpha > 0\n","        unknown = unknown - fg\n","        unknown = morphology.distance_transform_edt(unknown == 0) <= np.random.randint(1, 20)\n","        trimap = fg\n","        trimap[unknown] = 0.5\n","        # print(trimap[:, :, :1].shape)\n","        return trimap[:, :, :1]\n","\n","    def __len__(self):\n","        return len(self.imgs)\n","\n","    def __getitem__(self, idx):\n","        img = cv2.imread(os.path.join(self.root, './train/blurred_image', self.imgs[idx]))\n","        alpha = cv2.imread(os.path.join(self.root, './train/mask', self.alphas[idx]))\n","        h, w, c = img.shape\n","        rh = 512\n","        rw = int(w / h * 512)\n","        rh = rh - rh % 32  # 512\n","        rw = rw - rw % 32  # 896\n","        img = cv2.resize(img, (rw, rh))\n","        alpha = cv2.resize(alpha, (rw, rh))\n","        trimap = self.getTrimap(alpha)\n","        # print(trimap.shape)\n","        if self.transform:\n","            img = self.transform(img)\n","        alpha = self.tensor(alpha[:, :, 0])\n","        return self.imgs[idx], img, trimap, alpha"],"metadata":{"id":"mn1d1FkRdvCI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["blurer = GaussianBlurLayer(1, 3).cuda()\n","\n","\n","def supervised_training_iter(\n","        modnet, optimizer, image, trimap, gt_matte,\n","        semantic_scale=10.0, detail_scale=10.0, matte_scale=1.0):\n","\n","    global blurer\n","    # set the model to train mode and clear the optimizer\n","    modnet.train()\n","    optimizer.zero_grad()\n","\n","    # forward the model\n","    pred_semantic, pred_detail, pred_matte = modnet(image, False)\n","\n","    # calculate the boundary mask from the trimap\n","    boundaries = (trimap < 0.5) + (trimap > 0.5)\n","\n","    # calculate the semantic loss\n","    gt_semantic = F.interpolate(gt_matte, scale_factor=1 / 16, mode='bilinear')\n","    gt_semantic = blurer(gt_semantic)\n","    semantic_loss = torch.mean(F.mse_loss(pred_semantic, gt_semantic))\n","    semantic_loss = semantic_scale * semantic_loss\n","\n","    # calculate the detail loss\n","    pred_boundary_detail = torch.where(boundaries, trimap, pred_detail)\n","    gt_detail = torch.where(boundaries, trimap, gt_matte)\n","    detail_loss = torch.mean(F.l1_loss(pred_boundary_detail, gt_detail))\n","    detail_loss = detail_scale * detail_loss\n","\n","    # calculate the matte loss\n","    pred_boundary_matte = torch.where(boundaries, trimap, pred_matte)\n","    matte_l1_loss = F.l1_loss(pred_matte, gt_matte) + 4.0 * F.l1_loss(pred_boundary_matte, gt_matte)\n","    matte_compositional_loss = F.l1_loss(image * pred_matte, image * gt_matte) \\\n","                               + 4.0 * F.l1_loss(image * pred_boundary_matte, image * gt_matte)\n","    matte_loss = torch.mean(matte_l1_loss + matte_compositional_loss)\n","    matte_loss = matte_scale * matte_loss\n","\n","    # calculate the final loss, backward the loss, and update the model\n","    loss = semantic_loss + detail_loss + matte_loss\n","    loss.backward()\n","    optimizer.step()\n","\n","    # for test\n","    return semantic_loss, detail_loss, matte_loss"],"metadata":{"id":"Jh_rMJxUdz6h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main(root, resume=True, std=1):\n","    \"\"\" resume=True if not first runing else False  \"\"\"\n","    save_model_dir = 'ModelDefault'\n","    modnet = MODNet()\n","    modnet = nn.DataParallel(modnet)\n","    if resume:\n","        VModel = sorted(os.listdir(save_model_dir))[-1]\n","        pretrained_ckpt = os.path.join(save_model_dir, VModel)\n","    else:\n","        pretrained_ckpt = './000006.ckpt'\n","    print(pretrained_ckpt)\n","    logging.info(f\"model load {pretrained_ckpt}\")\n","    GPU = True if torch.cuda.device_count() > 0 else False\n","    if GPU:\n","        print('Use GPU...')\n","        modnet = modnet.cuda()\n","        # Comment next row out if you change the image size or you don't use the modle author provided else pass\n","        modnet.load_state_dict(torch.load(pretrained_ckpt))\n","    else:\n","        print('Use CPU...')\n","        # Comment next row out if you change the image size or you don't use the modle author provided else pass\n","        modnet.load_state_dict(torch.load(pretrained_ckpt, map_location=torch.device('cpu')))\n","    bs = 1 # batch size\n","    lr = 0.01  # learn rate\n","    epochs = 10  # total epochs\n","    num_workers = 32\n","    optimizer = torch.optim.SGD(modnet.parameters(), lr=lr, momentum=0.9)\n","    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10,\n","                                                   gamma=0.1)  \n","\n","    dataset = ImagesDataset(root, torch_transforms)\n","    dataloader = DataLoader(dataset, batch_size=bs, num_workers=num_workers, pin_memory=True)\n","\n","    for epoch in range(std, epochs+1):\n","        mattes = []\n","        for idx, (img_file, image, trimap, gt_matte) in enumerate(dataloader, start=1):\n","            trimap = np.transpose(trimap, (0, 3, 1, 2)).float().cuda()\n","            image = image.cuda()\n","            gt_matte = gt_matte.cuda()\n","            semantic_loss, detail_loss, matte_loss = supervised_training_iter(modnet, optimizer, image, trimap,\n","                                                                              gt_matte)\n","            info = f\"epoch: {epoch}/{epochs} semantic_loss: {semantic_loss}, detail_loss: {detail_loss}, matte_loss： {matte_loss}\"\n","            print(idx, info)\n","            logging.info(info)\n","            mattes.append(float(matte_loss))\n","        avg_matte = float(np.mean(mattes))\n","        logging.info(f\"epoch: {epoch}/{epochs}, average_matte_loss: {avg_matte}\")\n","        lr_scheduler.step()\n","        if epoch % 2 == 0:\n","            torch.save(modnet.state_dict(), os.path.join(save_model_dir, 'New_{:0>6d}.ckpt'.format(epoch)))\n","            print(f'------save model------{epoch}  {epoch}.ckpt')\n","            logging.info(f'------save model------{epoch}  {epoch}.ckpt')\n","            "],"metadata":{"id":"p1GGNQkOeLdC","executionInfo":{"status":"ok","timestamp":1655910604235,"user_tz":-420,"elapsed":337,"user":{"displayName":"Hữu Phương Nguyễn","userId":"15386454328741485337"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["\n","if __name__ == '__main__':\n","    path = './P3M-10k'\n","    # step1\n","    main(path)\n","   "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S1HXz9RBeOjl","outputId":"36633d78-3926-48df-cd8c-18cc529b70e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["./000006.ckpt\n","Use GPU...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["1 epoch: 1/10 semantic_loss: 0.10736218094825745, detail_loss: 0.042826198041439056, matte_loss： 0.04340601712465286\n","2 epoch: 1/10 semantic_loss: 0.02927524968981743, detail_loss: 0.04448661208152771, matte_loss： 0.042527295649051666\n","3 epoch: 1/10 semantic_loss: 0.011719055473804474, detail_loss: 0.0166544821113348, matte_loss： 0.012785042636096478\n","4 epoch: 1/10 semantic_loss: 0.08921056240797043, detail_loss: 0.023226376622915268, matte_loss： 0.08029352128505707\n","5 epoch: 1/10 semantic_loss: 0.022825002670288086, detail_loss: 0.004628631751984358, matte_loss： 0.011929030530154705\n","6 epoch: 1/10 semantic_loss: 0.0074950335547327995, detail_loss: 0.029699979349970818, matte_loss： 0.021257508546113968\n","7 epoch: 1/10 semantic_loss: 0.0270024836063385, detail_loss: 0.027360571548342705, matte_loss： 0.019940150901675224\n","8 epoch: 1/10 semantic_loss: 0.027666816487908363, detail_loss: 0.020751431584358215, matte_loss： 0.015498887747526169\n","9 epoch: 1/10 semantic_loss: 0.013257571496069431, detail_loss: 0.016518399119377136, matte_loss： 0.011624688282608986\n","10 epoch: 1/10 semantic_loss: 0.014746205881237984, detail_loss: 0.03601449728012085, matte_loss： 0.027154089882969856\n","11 epoch: 1/10 semantic_loss: 0.004782743752002716, detail_loss: 0.04031715542078018, matte_loss： 0.028111740946769714\n","12 epoch: 1/10 semantic_loss: 0.026625223457813263, detail_loss: 0.06927406042814255, matte_loss： 0.056890033185482025\n","13 epoch: 1/10 semantic_loss: 0.03546494245529175, detail_loss: 0.04624650627374649, matte_loss： 0.029371008276939392\n","14 epoch: 1/10 semantic_loss: 0.027317430824041367, detail_loss: 0.013959653675556183, matte_loss： 0.01210245955735445\n","15 epoch: 1/10 semantic_loss: 0.12104451656341553, detail_loss: 0.024572063237428665, matte_loss： 0.038306448608636856\n","16 epoch: 1/10 semantic_loss: 0.016463709995150566, detail_loss: 0.012652602046728134, matte_loss： 0.0130784772336483\n","17 epoch: 1/10 semantic_loss: 0.10098416358232498, detail_loss: 0.06694096326828003, matte_loss： 0.08739861845970154\n","18 epoch: 1/10 semantic_loss: 0.026903700083494186, detail_loss: 0.053592339158058167, matte_loss： 0.03950072452425957\n","19 epoch: 1/10 semantic_loss: 0.014988221228122711, detail_loss: 0.0856909528374672, matte_loss： 0.06582066416740417\n","20 epoch: 1/10 semantic_loss: 0.0023805967066437006, detail_loss: 0.005993149243295193, matte_loss： 0.005827527493238449\n","21 epoch: 1/10 semantic_loss: 0.09171079844236374, detail_loss: 0.12833476066589355, matte_loss： 0.10867971926927567\n","22 epoch: 1/10 semantic_loss: 0.013682977296411991, detail_loss: 0.027198951691389084, matte_loss： 0.022287379950284958\n","23 epoch: 1/10 semantic_loss: 0.006880750879645348, detail_loss: 0.06221660226583481, matte_loss： 0.043025679886341095\n","24 epoch: 1/10 semantic_loss: 0.030363304540514946, detail_loss: 0.0627521499991417, matte_loss： 0.04282427951693535\n","25 epoch: 1/10 semantic_loss: 0.005203215405344963, detail_loss: 0.00831566471606493, matte_loss： 0.005566777195781469\n","26 epoch: 1/10 semantic_loss: 0.02200889214873314, detail_loss: 0.030070889741182327, matte_loss： 0.022934600710868835\n","27 epoch: 1/10 semantic_loss: 0.013053633272647858, detail_loss: 0.0222897008061409, matte_loss： 0.014018108136951923\n","28 epoch: 1/10 semantic_loss: 0.024214092642068863, detail_loss: 0.0640910416841507, matte_loss： 0.0453486368060112\n","29 epoch: 1/10 semantic_loss: 0.02202632650732994, detail_loss: 0.01723559759557247, matte_loss： 0.013000916689634323\n","30 epoch: 1/10 semantic_loss: 0.003949443809688091, detail_loss: 0.004881025291979313, matte_loss： 0.0033411672338843346\n","31 epoch: 1/10 semantic_loss: 0.01096961461007595, detail_loss: 0.0246710404753685, matte_loss： 0.019157184287905693\n","32 epoch: 1/10 semantic_loss: 0.11510595679283142, detail_loss: 0.12467369437217712, matte_loss： 0.09305552393198013\n","33 epoch: 1/10 semantic_loss: 0.010615791194140911, detail_loss: 0.011156904511153698, matte_loss： 0.01091021578758955\n","34 epoch: 1/10 semantic_loss: 0.015920519828796387, detail_loss: 0.013114672154188156, matte_loss： 0.011220240965485573\n","35 epoch: 1/10 semantic_loss: 0.02735612541437149, detail_loss: 0.01649286039173603, matte_loss： 0.012557366862893105\n","36 epoch: 1/10 semantic_loss: 0.029235070571303368, detail_loss: 0.02771027758717537, matte_loss： 0.031330231577157974\n","37 epoch: 1/10 semantic_loss: 0.05509583279490471, detail_loss: 0.02818363904953003, matte_loss： 0.040825746953487396\n","38 epoch: 1/10 semantic_loss: 0.035423316061496735, detail_loss: 0.13433530926704407, matte_loss： 0.10722330957651138\n","39 epoch: 1/10 semantic_loss: 0.004672913812100887, detail_loss: 0.007999959401786327, matte_loss： 0.006427655927836895\n","40 epoch: 1/10 semantic_loss: 0.01804606430232525, detail_loss: 0.02822072058916092, matte_loss： 0.020224619656801224\n","41 epoch: 1/10 semantic_loss: 0.03267674520611763, detail_loss: 0.10909189283847809, matte_loss： 0.08853907883167267\n","42 epoch: 1/10 semantic_loss: 0.012271156534552574, detail_loss: 0.029103953391313553, matte_loss： 0.024503426626324654\n","43 epoch: 1/10 semantic_loss: 0.06902053952217102, detail_loss: 0.11831127107143402, matte_loss： 0.09281101822853088\n","44 epoch: 1/10 semantic_loss: 0.07491842657327652, detail_loss: 0.00884534977376461, matte_loss： 0.011150284670293331\n","45 epoch: 1/10 semantic_loss: 0.00368613563477993, detail_loss: 0.012638004496693611, matte_loss： 0.00889101903885603\n","46 epoch: 1/10 semantic_loss: 0.014533476904034615, detail_loss: 0.017369939014315605, matte_loss： 0.01524245273321867\n","47 epoch: 1/10 semantic_loss: 0.012019168585538864, detail_loss: 0.018359651789069176, matte_loss： 0.014799962751567364\n","48 epoch: 1/10 semantic_loss: 0.7391906976699829, detail_loss: 0.0578007772564888, matte_loss： 0.2085845172405243\n","49 epoch: 1/10 semantic_loss: 0.005913875065743923, detail_loss: 0.006227552890777588, matte_loss： 0.004819692112505436\n","50 epoch: 1/10 semantic_loss: 0.022229786962270737, detail_loss: 0.05977483466267586, matte_loss： 0.04176555201411247\n","51 epoch: 1/10 semantic_loss: 0.007487558759748936, detail_loss: 0.024360913783311844, matte_loss： 0.015305832028388977\n","52 epoch: 1/10 semantic_loss: 0.028420424088835716, detail_loss: 0.04101463407278061, matte_loss： 0.033963337540626526\n"]},{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fec4c3b4dd0>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1322, in _shutdown_workers\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 140, in join\n","    res = self._popen.wait(timeout)\n","  File \"/usr/lib/python3.7/multiprocessing/popen_fork.py\", line 45, in wait\n","    if not wait([self.sentinel], timeout):\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 921, in wait\n","    ready = selector.select(timeout)\n","  File \"/usr/lib/python3.7/selectors.py\", line 415, in select\n","    fd_event_list = self._selector.poll(timeout)\n","KeyboardInterrupt: \n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"wIbBi7VP5_hy"},"execution_count":null,"outputs":[]}]}